{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10816803,"sourceType":"datasetVersion","datasetId":6715531}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-22T08:14:45.238190Z","iopub.execute_input":"2025-02-22T08:14:45.238483Z","iopub.status.idle":"2025-02-22T08:14:46.254757Z","shell.execute_reply.started":"2025-02-22T08:14:45.238460Z","shell.execute_reply":"2025-02-22T08:14:46.253923Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/ipc-recommandation/secs.jsonl\n/kaggle/input/ipc-recommandation/schemas.json\n/kaggle/input/ipc-recommandation/dev.jsonl\n/kaggle/input/ipc-recommandation/test.jsonl\n/kaggle/input/ipc-recommandation/label_tree.json\n/kaggle/input/ipc-recommandation/citation_network.json\n/kaggle/input/ipc-recommandation/ils2v.bin\n/kaggle/input/ipc-recommandation/type_map.json\n/kaggle/input/ipc-recommandation/label_vocab.json\n/kaggle/input/ipc-recommandation/best_model.pt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Install dependencies\n!apt-get install -y git make g++\n\n# Clone the Sent2Vec repository\n!git clone https://github.com/epfml/sent2vec.git\n\n# Navigate into the repository\n%cd sent2vec\n\n# Compile the code\n!make","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T08:14:46.255859Z","iopub.execute_input":"2025-02-22T08:14:46.256317Z","iopub.status.idle":"2025-02-22T08:15:10.450180Z","shell.execute_reply.started":"2025-02-22T08:14:46.256291Z","shell.execute_reply":"2025-02-22T08:15:10.449297Z"}},"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\ng++ is already the newest version (4:11.2.0-1ubuntu1).\ng++ set to manually installed.\nmake is already the newest version (4.3-4.1build1).\nmake set to manually installed.\nSuggested packages:\n  gettext-base git-daemon-run | git-daemon-sysvinit git-doc git-email git-gui gitk gitweb git-cvs\n  git-mediawiki git-svn\nThe following packages will be upgraded:\n  git\n1 upgraded, 0 newly installed, 0 to remove and 128 not upgraded.\nNeed to get 3,165 kB of archives.\nAfter this operation, 4,096 B of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 git amd64 1:2.34.1-1ubuntu1.12 [3,165 kB]\nFetched 3,165 kB in 1s (2,259 kB/s)\n(Reading database ... 127400 files and directories currently installed.)\nPreparing to unpack .../git_1%3a2.34.1-1ubuntu1.12_amd64.deb ...\nUnpacking git (1:2.34.1-1ubuntu1.12) over (1:2.34.1-1ubuntu1.11) ...\nSetting up git (1:2.34.1-1ubuntu1.12) ...\nCloning into 'sent2vec'...\nremote: Enumerating objects: 425, done.\u001b[K\nremote: Counting objects: 100% (22/22), done.\u001b[K\nremote: Compressing objects: 100% (20/20), done.\u001b[K\nremote: Total 425 (delta 9), reused 4 (delta 1), pack-reused 403 (from 1)\u001b[K\nReceiving objects: 100% (425/425), 447.46 KiB | 13.56 MiB/s, done.\nResolving deltas: 100% (261/261), done.\n/kaggle/working/sent2vec\nc++ -pthread -std=c++0x -O3 -funroll-loops -c src/args.cc\nc++ -pthread -std=c++0x -O3 -funroll-loops -c src/dictionary.cc\nc++ -pthread -std=c++0x -O3 -funroll-loops -c src/productquantizer.cc\nc++ -pthread -std=c++0x -O3 -funroll-loops -c src/matrix.cc\nc++ -pthread -std=c++0x -O3 -funroll-loops -c src/shmem_matrix.cc\nc++ -pthread -std=c++0x -O3 -funroll-loops -c src/qmatrix.cc\nc++ -pthread -std=c++0x -O3 -funroll-loops -c src/vector.cc\nc++ -pthread -std=c++0x -O3 -funroll-loops -c src/model.cc\nc++ -pthread -std=c++0x -O3 -funroll-loops -c src/utils.cc\nc++ -pthread -std=c++0x -O3 -funroll-loops -c src/fasttext.cc\nc++ -pthread -std=c++0x -O3 -funroll-loops args.o dictionary.o productquantizer.o matrix.o shmem_matrix.o qmatrix.o vector.o model.o utils.o fasttext.o src/main.cc -o fasttext -lrt\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install .","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T08:15:10.451750Z","iopub.execute_input":"2025-02-22T08:15:10.452078Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/working/sent2vec\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: Cython>=0.29.13 in /usr/local/lib/python3.10/dist-packages (from sent2vec==0.0.0) (3.0.11)\nRequirement already satisfied: numpy>=1.17.1 in /usr/local/lib/python3.10/dist-packages (from sent2vec==0.0.0) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.1->sent2vec==0.0.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.1->sent2vec==0.0.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.1->sent2vec==0.0.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.1->sent2vec==0.0.0) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.1->sent2vec==0.0.0) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.1->sent2vec==0.0.0) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17.1->sent2vec==0.0.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17.1->sent2vec==0.0.0) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.1->sent2vec==0.0.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17.1->sent2vec==0.0.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17.1->sent2vec==0.0.0) (2024.2.0)\nBuilding wheels for collected packages: sent2vec\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# !git clone https://github.com/Law-AI/LeSICiN.git\n!rm -rf /kaggle/working/sent2vec\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sent2vec\nsent2vec.Sent2vecModel()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch_geometric\n\n# Optional dependencies:\n!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.5.0+cu124.html","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch_sparse\nimport numpy as np # linear algebra\nimport pandas as pd\nimport copy\nimport torch_scatter\nimport numpy as np\nimport torch\nimport numpy as np\nimport string\nimport copy\nimport multiprocessing as mp\nfrom tqdm import tqdm\nimport json\nimport pickle as pkl\nimport torch\nimport torch_sparse\nfrom tqdm import tqdm\nfrom collections import defaultdict\nimport torch\nimport random\nfrom tqdm import tqdm\nfrom functools import partial","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hc = {\n\t\"seed\": 42,\n\t\"do_train_dev\": False,\n\t\"do_test\": True,\n\t\"do_infer\": True,\n\t\"vocab_limit\": None,\n\t\"vocab_thresh\": None,\n\t\"weight_scheme\": \"tws\",\n\t\"tws_thresh\": 10.0,\n\t\"train_bs\": 64,\n\t\"dev_bs\": 512,\n\t\"test_bs\": 512,\n\t\"infer_bs\": 512,\n\t\"max_segments\": 128,\n\t\"max_segment_size\": None,\n\t\"num_mpath_samples\": 8,\n\t\"hidden_size\": 200,\n\t\"opt_lr\": 1e-4,\n\t\"opt_wt_decay\": 1e-4,\n\t\"sch_factor\": 0.2,\n\t\"sch_patience\": 3,\n\t\"num_epochs\": 2,\n\t\"pthresh\": 0.51,\n\t\"thetas\": [1,2,3],\n\t\"lambdas\": [0.25,0.75],\n\t\"dropout\": 0.1\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dc = {\n\t\"sec_src\": \"/kaggle/input/ipc-recommandation/secs.jsonl\",\n\t\"train_src\": \"/kaggle/input/ipc-recommandation/train.jsonl\",\n\t\"dev_src\": \"/kaggle/input/ipc-recommandation/dev.jsonl\",\n\t\"test_src\": \"/kaggle/input/ipc-recommandation/test.jsonl\",\n\t\"infer_src\": None,\n\t\"sec_cache\": \"/kaggle/working/secs.pkl\",\n\t\"train_cache\": \"/kaggle/working/train.pkl\",\n\t\"dev_cache\": \"/kaggle/working/dev.pkl\",\n\t\"test_cache\": \"/kaggle/working/test.pkl\",\n\t\"infer_cache\": \"/kaggle/working/infer.pkl\",\n\t\"s2v_path\": \"/kaggle/input/ipc-recommandation/ils2v.bin\",\n\t\"type_map\": \"/kaggle/input/ipc-recommandation/type_map.json\",\n\t\"label_tree\": \"/kaggle/input/ipc-recommandation/label_tree.json\",\n\t\"citation_network\": \"/kaggle/input/ipc-recommandation/citation_network.json\",\n\t\"schemas\": \"/kaggle/input/ipc-recommandation/schemas.json\",\n\t\"model_load\": \"/kaggle/input/ipc-recommandation/best_model.pt\",\n\t\"metrics_load\": None,\n\t\"model_dump\": \"/kaggle/input/ipc-recommandation/best_model.pt\",\n\t\"dev_metrics_dump\": \"/kaggle/input/ipc-recommandation/dev_metrics.json\",\n\t\"test_metrics_dump\": \"/kaggle/input/ipc-recommandation/test_metrics.json\",\n\t\"infer_trg\": None\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nclass LstmNet(torch.nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.lstm = torch.nn.LSTM(hidden_size, hidden_size // 2, batch_first=True, bidirectional=True)\n\n    def forward(self, inputs, mask=None): # [B, S, H], [B, S]\n        mask = mask if mask is not None else torch.ones(inputs.size(0), inputs.size(1), device=inputs.device)\n        lengths = mask.sum(dim=-1) # [B,]\n\n        # need to pack inputs before passing to RNN and unpack obtained outputs\n        pck_inputs = torch.nn.utils.rnn.pack_padded_sequence(inputs, lengths.cpu(), batch_first=True, enforce_sorted=False)\n        pck_hidden_all = self.lstm(pck_inputs)[0]\n        hidden_all = torch.nn.utils.rnn.pad_packed_sequence(pck_hidden_all, batch_first=True)[0] # [B, S, H]\n\n        return hidden_all\n\nclass AttnNet(torch.nn.Module):\n    def __init__(self, hidden_size, drop=0.1):\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        self.attn_fc = torch.nn.Linear(hidden_size, hidden_size)\n        self.context = torch.nn.Parameter(torch.rand(hidden_size))\n\n        self.dropout = torch.nn.Dropout(drop)\n\n    def forward(self, inputs, mask=None, dyn_context=None): # [B, S, H], [B, S], [B, H]\n\n        mask = mask if mask is not None else torch.ones(inputs.size(0), inputs.size(1), device=inputs.device)\n        # use static (learned) context vector if dynamic context is unavailable\n        context = dyn_context if dyn_context is not None else self.context.expand(inputs.size(0), self.hidden_size) # [B, H]\n\n        act_inputs = torch.tanh(self.dropout(self.attn_fc(inputs)))\n\n        scores = torch.bmm(act_inputs, context.unsqueeze(2)).squeeze(2) # [B, S]\n        msk_scores = scores.masked_fill((1 - mask).bool(), -1e-32)\n        msk_scores = torch.nn.functional.softmax(msk_scores, dim=1)\n\n        hidden = torch.sum(inputs * msk_scores.unsqueeze(2), dim=1) # [B, H]\n        return hidden\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass LSIDataset(torch.utils.data.Dataset):\n    def __init__(self, jsonl_file=None, data_list=None):\n        super().__init__()\n\n        self.annotated = False\n        self.sent_vectorized = False\n        self.dataset = []\n        if data_list is not None:\n            self.dataset = copy.deepcopy(data_list)\n            for instance in tqdm(self.dataset, desc=\"Loading data from list\"):\n                instance['text'] = np.array(instance['text'])\n                if 'labels' in instance:\n                    self.annotated = True\n                    instance['labels'] = np.array(instance['labels'])\n\n        elif jsonl_file is not None:\n            self.dataset = []\n            with open(jsonl_file) as fr:\n                for line in tqdm(fr, desc=\"Loading data from file\"):\n                    doc = json.loads(line)\n                    text = np.array([sent for sent in doc['text']])\n                    newdoc = {'id': doc['id'], 'text': text}\n                    if 'labels' in doc:\n                        self.annotated = True\n                        labels = np.array(doc['labels'])\n                        newdoc['labels'] = labels\n                    self.dataset.append(newdoc)\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, index):\n        return self.dataset[index]\n\n    def save_data(self, data_file):\n        # if not os.path.exists(data_file):\n\n        with open(data_file, 'wb') as fw:\n            pkl.dump(self, fw)\n\n    def load_data(data_file):\n        with open(data_file, 'rb') as fr:\n            return pkl.load(fr)\n\n    # remove puncutations and empty sentences\n    def preprocess(self):\n        for i, instance in enumerate(tqdm(self.dataset, desc=\"Preprocessing\")):\n            text = []\n            # print(f\"{instance['text']} helllllllll {len(instance['text'])}\")\n            for j, sent in enumerate(instance['text']):\n                ppsent = sent.strip().lower().translate(str.maketrans('', '', string.punctuation))\n                # print(ppsent)\n                if len(ppsent.split()) > 1:\n                    text.append(ppsent)\n            instance['text'] = np.array(text)\n\n    # break each sentence string into word tokens\n    def tokenize(self):\n        for i, instance in enumerate(tqdm(self.dataset, desc=\"Tokenizing\")):\n            text = []\n            for j, sent in enumerate(instance['text']):\n                toksent = np.array(sent.strip().split())\n                text.append(toksent)\n            instance['text'] = np.array(text, dtype=object)\n\n    # generate a vector for each sentence using Sent2Vec\n    def sent_vectorize(self, sent2vec_model):\n        # print(self.dataset)\n        for i, instance in enumerate(tqdm(self.dataset, desc=\"Embedding sentences\")):\n            esents = sent2vec_model.embed_sentences(instance['text'])\n            instance['text'] = np.delete(esents, np.where(esents.sum(axis=1) == 0)[0], axis=0)\n        self.sent_vectorized = True\n\n\n# unified code for generating mini batches of data for both facts and sections during train / dev / test / inference\nclass MiniBatch:\n    def __init__(self, examples, vocab=None, label_vocab=None, schemas=None, type_map=None, node_vocab=None, edge_vocab=None, adjacency=None, hidden_size=200, max_segments=4, max_segment_size=8, num_mpath_samples=2):\n        # provide vocab if not sent vectorized, None otherwise\n        self.sent_vectorized = True if vocab is None else False\n        # provide label_vocab if annotated, None otherwise\n        # self.annotated = True if label_vocab is not None else False\n        self.annotated =  False\n        # provide graph data if struct encoder is to be used on these examples, None otherwise\n        self.sample_metapaths = True if schemas is not None else False\n\n        self.max_segments = max_segments\n\n        if not self.sent_vectorized:\n            self.vocab = vocab\n            self.max_segment_size = max_segment_size\n        else:\n            self.sent_hidden_size = hidden_size\n\n        if self.annotated:\n            self.label_vocab = label_vocab\n\n        if self.sample_metapaths:\n            self.schemas = schemas\n            self.type_map = type_map\n            self.node_vocab = node_vocab\n            self.edge_vocab = edge_vocab\n            self.adjacency = adjacency\n            self.num_mpath_samples = num_mpath_samples\n\n        max_len = max([len(d['text']) for d in examples])\n        max_segments = min(self.max_segments, max_len)\n\n        # expected shape of text tensors\n        if not self.sent_vectorized:\n            max_segment_size = min(self.max_segment_size, max([len(s) for d in examples for s in d['text']]))\n            self.tokens = torch.zeros(len(examples), max_segments, max_segment_size, dtype=torch.long) # [D, S, W]\n        else:\n            self.doc_inputs = torch.zeros(len(examples), max_segments, self.sent_hidden_size) # [D, S, H]\n\n        self.example_ids = []\n\n        if self.annotated:\n            # expected shape of true label indicator tensors\n            self.labels = torch.zeros(len(examples), len(self.label_vocab)) # [D, C]\n\n        for i, instance in enumerate(examples):\n            if not self.sent_vectorized:\n                for j, sent in enumerate(instance['text']):\n                    # fill up the j-th sentence of i-th example with word tokens\n                    self.tokens[i, j, :len(sent)] = torch.from_numpy(np.array([self.vocab[w] for w in sent]))\n            else:\n                # fill up the i-th example with sentence embeddings\n                self.doc_inputs[i, :len(instance['text']), :] = torch.from_numpy(instance['text'])[:max_segments]\n\n            self.example_ids.append(instance['id'])\n\n            if self.annotated:\n                label_list = torch.from_numpy(np.array([self.label_vocab[l] for l in instance['labels']]))\n                self.labels[i].scatter_(0, label_list, 1.)\n\n        if not self.sent_vectorized:\n            self.mask = (self.tokens != 0).float() # [D, S, W]\n        else:\n            self.mask = (self.doc_inputs != 0).any(dim=2).float() # [D, S]\n\n        if self.sample_metapaths:\n            trg_node_tokens = torch.tensor([self.node_vocab[self.type_map[x]][x] for x in self.example_ids])\n            self.node_tokens, self.edge_tokens = self.generate_metapaths(trg_node_tokens, self.schemas, self.adjacency, self.edge_vocab, num_samples=self.num_mpath_samples) # N * [M, D, L+1], N * [M, D, L]\n\n    # sample metapaths using adjacency matrices\n    def generate_metapaths(self, indices, schemas, adjacency, edge_vocab, num_samples=2): # [D,]\n        indices = indices.repeat(num_samples) # [M*D,]\n\n        tokens, edge_tokens = [], []\n\n        # repeat over all schemas\n        for i in range(len(schemas)):\n            ins_tokens, ins_edge_tokens = [indices], []\n            for keys in schemas[i]:\n                neighbours = adjacency[keys].sample(num_neighbors=1, subset=ins_tokens[-1]).squeeze(1) # [M*D,]\n                relations = torch.full(neighbours.shape, edge_vocab[keys[1]], dtype=torch.long) # [M*D,]\n\n                ins_tokens.append(neighbours)\n                ins_edge_tokens.append(relations)\n\n            ins_tokens = torch.stack(ins_tokens, dim=1)\n            ins_tokens = ins_tokens.view(num_samples, -1, ins_tokens.size(1)) # [M, D, L+1]\n\n            ins_edge_tokens = torch.stack(ins_edge_tokens, dim=1)\n            ins_edge_tokens = ins_edge_tokens.view(num_samples, -1, ins_edge_tokens.size(1)) # [M, D, L]\n\n            tokens.append(ins_tokens)\n            edge_tokens.append(ins_edge_tokens)\n\n        return tokens, edge_tokens\n\n    # automatic memory pinning for faster cpu to cuda transfer\n    def pin_memory(self):\n        if not self.sent_vectorized:\n            self.tokens.pin_memory()\n        else:\n            self.doc_inputs.pin_memory()\n        self.mask.pin_memory()\n        if self.annotated:\n            self.labels.pin_memory()\n        if self.sample_metapaths:\n            for i in range(len(self.node_tokens)):\n                self.node_tokens[i].pin_memory()\n                self.edge_tokens[i].pin_memory()\n        return self\n\n    # transfer pinned cpu tensors to cuda\n    def cuda(self, dev='cuda'):\n        if not self.sent_vectorized:\n            self.tokens = self.tokens.cuda(dev, non_blocking=True)\n        else:\n            self.doc_inputs = self.doc_inputs.cuda(dev, non_blocking=True)\n        self.mask = self.mask.cuda(dev, non_blocking=True)\n        if self.annotated:\n            self.labels = self.labels.cuda(dev, non_blocking=True)\n        if self.sample_metapaths:\n            for i in range(len(self.node_tokens)):\n                self.node_tokens[i] = self.node_tokens[i].cuda(dev, non_blocking=True)\n                self.edge_tokens[i] = self.edge_tokens[i].cuda(dev, non_blocking=True)\n        return self\n\ndef collate_func(examples, **kwargs):\n    return MiniBatch(examples, **kwargs)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n# from model.basicmodules import LstmNet, AttnNet\n\nclass HierAttnNet(torch.nn.Module):\n    def __init__(self, hidden_size, vocab_size=None, drop=0.1):\n        super().__init__()\n\n        if vocab_size is not None:\n            self.word_embedding = torch.nn.Embedding(vocab_size, hidden_size)\n            self.sent_lstm = LstmNet(hidden_size)\n            self.sent_attn = AttnNet(hidden_size, drop=drop)\n\n        self.doc_lstm = LstmNet(hidden_size)\n        self.doc_attn = AttnNet(hidden_size, drop=drop)\n\n    def forward(self, tokens=None, doc_inputs=None, mask=None, sent_dyn_context=None, doc_dyn_context=None): # [B, S, W], [B, S, H], [B, S, W] / [B, S], [B, S, H], [B, H]\n        if tokens is not None:\n            sent_inputs = self.word_embedding(tokens)\n\n            # flatten to 3-D\n            sent_inputs = sent_inputs.view(-1, sent_inputs.size(2), sent_inputs.size(3))\n            sent_mask = mask.view(-1, mask.size(2))\n\n            if sent_dyn_context is not None:\n                sent_dyn_context = sent_dyn_context.view(-1, sent_dyn_context.size(2))\n\n            sent_hidden_all = self.sent_lstm(sent_inputs, sent_mask)\n            sent_hidden = self.sent_attn(sent_hidden_all, sent_mask, dyn_context=sent_dyn_context)\n\n            doc_inputs = sent_hidden.view(tokens.size(0), tokens.size(1), -1)\n            doc_mask = (mask.sum(dim=2) > 0).float()\n        else:\n            doc_mask = mask\n\n        doc_hidden_all = self.doc_lstm(doc_inputs, doc_mask)\n        doc_hidden = self.doc_attn(doc_hidden_all, doc_mask, dyn_context=doc_dyn_context)\n        return doc_hidden\n\nclass MetapathAggrNet(torch.nn.Module):\n    def __init__(self, node_vocab_size, edge_vocab_size, hidden_size, drop=0.1, gdel=14.):\n        super().__init__()\n        self.emb_range = gdel / hidden_size\n\n        self.node_embedding = torch.nn.ModuleDict({ntype: torch.nn.Embedding(num_nodes, hidden_size) for ntype, num_nodes in node_vocab_size.items()})\n        for ntype, ntype_weights in self.node_embedding.items():\n            ntype_weights.weight.data.uniform_(- self.emb_range, self.emb_range)\n\n        self.scale_fc = torch.nn.ModuleDict({ntype: torch.nn.Linear(hidden_size, hidden_size) for ntype in node_vocab_size})\n\n        self.edge_embedding = torch.nn.Embedding(edge_vocab_size, hidden_size // 2)\n        self.edge_embedding.weight.data.uniform_(- self.emb_range, self.emb_range)\n\n        self.intra_attention = AttnNet(2 * hidden_size, drop=drop)\n\n        self.inter_fc = torch.nn.Linear(2 * hidden_size, 2 * hidden_size)\n        self.inter_context = torch.nn.Parameter(torch.rand(2 * hidden_size))\n\n        self.output_fc = torch.nn.Linear(2 * hidden_size, hidden_size)\n\n        self.dropout = torch.nn.Dropout(drop)\n\n    # Embed each node index using the node embedding matrix and then scale to generate same sized embeddings for each node type\n    def embed_and_scale(self, tokens, edge_tokens, schema): # [B, L+1], [B, L]\n        inputs, edge_inputs = [], []\n\n        node_type = schema[0][0]\n        node_input = self.dropout(self.node_embedding[node_type](tokens[:, 0])) # [B, H]\n        inputs.append(self.dropout(self.scale_fc[node_type](node_input)))\n\n        for i in range(edge_tokens.size(1)):\n            node_type = schema[i][2]\n            node_input = self.dropout(self.node_embedding[node_type](tokens[:, i+1])) # [B, H]\n            inputs.append(self.dropout(self.scale_fc[node_type](node_input)))\n\n            edge_inputs.append(self.dropout(self.edge_embedding(edge_tokens[:, i])))\n        inputs = torch.stack(inputs, dim=1) # [B, L+1, H]\n        edge_inputs = torch.stack(edge_inputs, dim=1) # [B, L, H]\n        return inputs, edge_inputs\n\n    # We are following the official implementation of the RotatE algorithm --- https://github.com/DeepGraphLearning/KnowledgeGraphEmbedding\n    def rotational_encoding(self, inputs, edge_inputs): # [B, L+1, H], [B, L, H/2]\n        PI = 3.14159265358979323846\n        hidden = inputs.clone()\n        for i in reversed(range(edge_inputs.size(1))):\n            hid_real, hid_imag = torch.chunk(hidden.clone()[:, i+1:, :], 2, dim=2) # [B, L-i, H/2], [B, L-i, H/2]\n            inp_real, inp_imag = torch.chunk(inputs[:, i, :], 2, dim=1) # [B, H/2], [B, H/2]\n\n            edge_complex = edge_inputs[:, i, :] / (self.emb_range / PI)\n            edge_real, edge_imag = torch.cos(edge_inputs[:, i, :]), torch.sin(edge_inputs[:, i, :]) # [B, H/2], [B, H/2]\n\n            out_real = inp_real.unsqueeze(1) + edge_real.unsqueeze(1) * hid_real - edge_imag.unsqueeze(1) * hid_imag # [B, L-i, H/2]\n            out_imag = inp_imag.unsqueeze(1) + edge_imag.unsqueeze(1) * hid_real + edge_real.unsqueeze(1) * hid_imag # [B, L-i, H/2]\n\n            hidden[:, i+1:, :] = torch.cat([out_real, out_imag], dim=2)\n        path_lens = 1 + torch.arange(hidden.size(1), device=hidden.device) # [L+1]\n        return hidden / path_lens.unsqueeze(0).unsqueeze(2)\n\n    def forward(self, tokens, edge_tokens, schemas, intra_context=None, inter_context=None):\n        hidden = []\n\n        # serially perform intra-metapath aggregation across the different schemas\n        for i in range(len(tokens)):\n            # flatten out the multiple samples of the same schema\n            mpath_tokens = tokens[i].view(-1, tokens[i].size(2)) # [M*D, L+1]\n            mpath_edge_tokens = edge_tokens[i].view(-1, edge_tokens[i].size(2)) # [M*D, L]\n\n            mpath_inputs, mpath_edge_inputs = self.embed_and_scale(mpath_tokens, mpath_edge_tokens, schemas[i])\n\n            mpath_hidden_all = self.rotational_encoding(mpath_inputs, mpath_edge_inputs) # [M*D, L+1, H]\n\n            # the first element in the sequence is the target node, the rest are transformed embeddings for other nodes in the metapath\n            mpath_hidden_all = torch.cat([mpath_hidden_all[:, 0, :].unsqueeze(1).repeat(1, mpath_hidden_all.size(1) - 1, 1), mpath_hidden_all[:, 1:, :]], dim=2) # [M*D, L, 2H]\n            mpath_hidden = torch.relu(self.intra_attention(mpath_hidden_all, dyn_context=intra_context)) # [M*D, 2H]\n\n            # aggregate transformed embeddings from multiple samples of the same schema\n            mpath_hidden = torch.sum(mpath_hidden.view(tokens[i].size(0), tokens[i].size(1), -1), dim=0) # [D, 2H]\n            hidden.append(mpath_hidden)\n        hidden = torch.stack(hidden, dim=1) # [D, N, 2H]\n\n        # perform inter-metapath aggregation across transformed embeddings for each schema\n        hidden_act = torch.mean(torch.tanh(self.dropout(self.inter_fc(hidden))), dim=0).expand_as(hidden) # [D, N, 2H]\n        context = self.inter_context.unsqueeze(0).repeat(hidden_act.size(0), 1).unsqueeze(2) if inter_context is None else inter_context.unsqueeze(2)\n        scores = torch.bmm(hidden_act, context) # [D, N, 1]\n\n        outputs = torch.sum(hidden * scores, dim=1) # [D, 2H]\n        outputs = self.dropout(self.output_fc(outputs)) # [D, H]\n\n        return outputs\n\nclass MatchNet(torch.nn.Module):\n    def __init__(self, hidden_size, num_labels, drop=0.1):\n        super().__init__()\n\n        self.match_lstm = LstmNet(hidden_size)\n        self.match_attn = AttnNet(hidden_size, drop=drop)\n        self.match_fc = torch.nn.Linear(2 * hidden_size, num_labels)\n\n        self.dropout = torch.nn.Dropout(drop)\n\n    def forward(self, fact_inputs, sec_inputs, context=None): # [D, H], [C, H]\n        sec_inputs = sec_inputs.expand(fact_inputs.size(0), sec_inputs.size(0), sec_inputs.size(1)) # [D, C, H]\n\n        sec_hidden_all = self.match_lstm(sec_inputs) # [D, C, H]\n        sec_hidden = self.match_attn(sec_hidden_all, dyn_context=context) # [D, H]\n\n        logits = self.dropout(self.match_fc(torch.cat([fact_inputs, sec_hidden], dim=1))) # [D, C]\n        scores = torch.sigmoid(logits).detach() # [D, C]\n        return logits, scores\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n\n\n\n# from model.submodules import HierAttnNet, MetapathAggrNet, MatchNet\n\n\nclass LeSICiN(torch.nn.Module):\n    def __init__(self, hidden_size, num_labels, node_vocab_size, edge_vocab_size, vocab_size=None, label_weights=None, pthresh=0.55, lambdas=(0.5, 0.5), thetas=(3, 2, 3), drop=0.1):\n        super().__init__()\n\n        self.text_encoder = HierAttnNet(hidden_size, vocab_size=vocab_size)\n        self.graph_encoder = MetapathAggrNet(node_vocab_size, edge_vocab_size, hidden_size)\n        self.match_network = MatchNet(hidden_size, num_labels)\n\n        self.match_context_transform = torch.nn.Linear(hidden_size, hidden_size)\n        self.intra_context_transform = torch.nn.Linear(hidden_size, 2 * hidden_size) # We need double the hidden size for Struct Encoder dynamic context\n        self.inter_context_transform = torch.nn.Linear(hidden_size, 2 * hidden_size)\n\n        self.criterion = torch.nn.BCEWithLogitsLoss(pos_weight=label_weights)\n\n        self.pred_threshold = pthresh\n        self.lambdas = lambdas # weights for scores\n        self.thetas = thetas # weights for losses\n        self.dropout = torch.nn.Dropout(drop)\n\n    def calculate_losses(self, logits_list, labels):\n        loss = 0\n        for i, logits in enumerate(logits_list):\n            if logits is not None:\n                loss += self.thetas[i] * self.criterion(logits, labels)\n        return loss\n\n    def forward(self, fact_batch, sec_batch, pthresh=None): # We have D documents in fact_batch and C sections in sec_batch\n        if pthresh is not None:\n            self.pred_threshold = pthresh\n\n        # Encode fact text using HAN\n        if not fact_batch.sent_vectorized:\n            fact_attr_hidden = self.text_encoder(tokens=fact_batch.tokens, mask=fact_batch.mask) # [D, H]\n        else:\n            fact_attr_hidden = self.text_encoder(doc_inputs=fact_batch.doc_inputs, mask=fact_batch.mask) # [D, H]\n\n        # Encode sec text using HAN\n        if not sec_batch.sent_vectorized:\n            sec_attr_hidden = self.text_encoder(tokens=sec_batch.tokens, mask=sec_batch.mask) # [C, H]\n        else:\n            sec_attr_hidden = self.text_encoder(doc_inputs=sec_batch.doc_inputs, mask=sec_batch.mask) # [C, H]\n\n        # context vector for matching with fact attributes\n        attr_match_context = self.dropout(self.match_context_transform(fact_attr_hidden)) # [D, H]\n\n        # Attribute scores\n        attr_logits, attr_scores = self.match_network(fact_attr_hidden, sec_attr_hidden, context=attr_match_context)\n\n        # sec-side context vectors for Struct Encoder\n        sec_intra_context = self.dropout(self.intra_context_transform(sec_attr_hidden)).repeat(sec_batch.num_mpath_samples, 1) # [M*C, H]\n        sec_inter_context = self.dropout(self.inter_context_transform(sec_attr_hidden)) # [C, H]\n\n        # Encode sec graph using MAGNN\n        sec_struct_hidden = self.graph_encoder(sec_batch.node_tokens, sec_batch.edge_tokens, sec_batch.schemas, intra_context=sec_intra_context, inter_context=sec_inter_context) # [C, H]\n\n        # Alignment scores\n        align_logits, align_scores = self.match_network(fact_attr_hidden, sec_struct_hidden, context=attr_match_context)\n\n        if fact_batch.sample_metapaths:\n            # fact-side context vectors for Struct Encoder\n            fact_intra_context = self.dropout(self.intra_context_transform(fact_attr_hidden)).repeat(fact_batch.num_mpath_samples, 1) # [M*D, H]\n            fact_inter_context = self.dropout(self.inter_context_transform(fact_attr_hidden)) # [D, H]\n\n            # Encode sec graph using MAGNN\n            fact_struct_hidden = self.graph_encoder(fact_batch.node_tokens, fact_batch.edge_tokens, fact_batch.schemas, intra_context=fact_intra_context, inter_context=fact_inter_context) # [D, H]\n\n            # context vector for matching with fact structure\n            struct_match_context = self.dropout(self.match_context_transform(fact_struct_hidden)) # [D, H]\n\n            # Structural scores\n            struct_logits, struct_scores = self.match_network(fact_struct_hidden, sec_struct_hidden, context=struct_match_context)\n\n        else:\n            struct_logits = None\n\n        # Combine scores and losses\n        scores = (self.lambdas[0] * attr_scores + self.lambdas[-1] * align_scores)\n        predictions = (scores > self.pred_threshold).float()\n\n        if fact_batch.annotated:\n            loss = self.calculate_losses([attr_logits, struct_logits, align_logits], fact_batch.labels)\n        else:\n            loss = None\n\n        return loss, predictions\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch_sparse\nfrom tqdm import tqdm\nfrom collections import defaultdict\nimport os\nimport pickle as pkl\n# from data_helper import MiniBatch\n\n# Create word vocab (if not sent vectorized) and label vocab from the specific order in the Section dataset\n# def generate_vocabs(train_data, label_data, limit=30000, thresh=1):\n#     if not train_data.sent_vectorized:\n#         freqs = defaultdict(int)\n#         for instance in tqdm(train_data.dataset + label_data.dataset, desc=\"Creating vocabulary\"):\n#             for sent in instance['text']:\n#                 for word in sent:\n#                     freqs[word] += 1\n#         vocab_set = set(w for w, f in freqs.items() if f >= thresh)\n#         vocab = {k: i for i, k in enumerate(vocab_set)}\n#     else:\n#         vocab = None\n\n#     label_vocab = {}\n#     for instance in tqdm(label_data.dataset, desc=\"Creating label vocabulary\"):\n#         label_vocab[instance['id']] = len(label_vocab)\n\n#     return vocab, label_vocab\n\ndef generate_vocabs(train_data, label_data, limit=30000, thresh=1, label_vocab_file='/kaggle/input/ipc-recommandation/label_vocab.json'):\n    # Ensure thresh has a valid numeric value.\n    # if thresh is None:\n    #     thresh = 1\n\n    # Compute word vocabulary only if not already sentence vectorized.\n    if not train_data.sent_vectorized:\n        freqs = defaultdict(int)\n        for instance in tqdm(train_data.dataset + label_data.dataset, desc=\"Creating vocabulary\"):\n            for sent in instance['text']:\n                for word in sent:\n                    freqs[word] += 1\n        vocab_set = set(w for w, f in freqs.items() if f >= thresh)\n        vocab = {k: i for i, k in enumerate(vocab_set)}\n    else:\n        vocab = None\n\n    # If a label_vocab file is provided and exists, load it; otherwise, compute it.\n    # if label_vocab_file is not None and os.path.exists(label_vocab_file):\n    #     with open(label_vocab_file, 'r') as fr:\n    #         label_vocab = pkl.load(fr)\n    # else:\n    label_vocab = {}\n    for instance in tqdm(label_data.dataset, desc=\"Creating label vocabulary\"):\n        label_vocab[instance['id']] = len(label_vocab)\n\n    return vocab, label_vocab\n\n\n# Create the entire graph by combining the label tree and fact-sec citation network\ndef generate_graph(label_vocab, type_map, label_tree_edges, cit_net_edges, label_name='section'):\n    node_vocab = defaultdict(dict) # each key is a node_type, and each value is a dict storing the vocab of all nodes under given node type\n    node_vocab[label_name] = label_vocab # manually set this since we want the label vocab to be consistent with node vocab for labels\n\n    edge_vocab = {}\n    edge_indices = defaultdict(list) # each key is a tuple (src node type, relationship name, trg node type), and each value is a list storing the edges from src node type to trg node type\n\n    for (node_a, edge_type, node_b) in label_tree_edges + cit_net_edges:\n        # first get the node type\n        node_a_type, node_b_type = type_map[node_a], type_map[node_b]\n\n        # create new vocab entries for edges and nodes\n        if edge_type not in edge_vocab:\n            edge_vocab[edge_type] = len(edge_vocab)\n\n        if node_a not in node_vocab[node_a_type]:\n            node_vocab[node_a_type][node_a] = len(node_vocab[node_a_type])\n        if node_b not in node_vocab[node_b_type]:\n            node_vocab[node_b_type][node_b] = len(node_vocab[node_b_type])\n\n        # get node indices\n        node_a_token = node_vocab[node_a_type][node_a]\n        node_b_token = node_vocab[node_b_type][node_b]\n\n        edge_indices[(node_a_type, edge_type, node_b_type)].append([node_a_token, node_b_token])\n\n    num_nodes = {ntype: len(nodes) for ntype, nodes in node_vocab.items()}\n\n    # same as edge_indices except that the edges under each key are now stored as sparse matrices\n    adjacency = {}\n    for keys, edges in edge_indices.items():\n        row, col = torch.tensor(edges).t()\n        sizes = (num_nodes[keys[0]], num_nodes[keys[-1]])\n        adj = torch_sparse.SparseTensor(row=row, col=col, sparse_sizes=sizes)\n        adjacency[tuple(keys)] = adj\n\n    return node_vocab, edge_vocab, edge_indices, adjacency\n\n# create label weights for BCE Loss since we have unbalanced class distribution\ndef generate_label_weights(train_data, label_vocab, dev='cuda:0', scheme=\"tws\", thresh=10.):\n    pos = torch.zeros(len(label_vocab), device=dev)\n    for instance in tqdm(train_data, desc=\"Generating label weights\"):\n        for l in instance['labels']:\n            pos[label_vocab[l]] += 1\n    weights = torch.clamp(pos.max() / pos, max=thresh) if scheme == 'tws' else len(train_data) / pos\n    return weights\n\n# Unified code to deal with a single train / dev / test / inference pass over the dataset\ndef train_dev_passs(model, optimizer, fact_loader, sec_batch, metrics=None, pred_threshold=None, train=False, infer=False, label_vocab=False):\n\n    model.train() if train else model.eval()\n\n    if infer:\n        outputs = []\n        inv_label_vocab = {v: k for k, v in label_vocab.items()}\n\n    for i, fact_batch in enumerate(tqdm(fact_loader, desc=\"Flowing data through model\")):\n        torch.cuda.empty_cache()\n\n        loss, predictions = model(fact_batch.cuda(), sec_batch.cuda(), pthresh=pred_threshold)\n\n        if train:\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n        if not infer:\n            batch_loss = loss.item()\n            metrics(predictions, fact_batch.labels, loss=batch_loss)\n\n        else:\n            for i, instance_preds in enumerate(predictions):\n                # gather true predictions\n                pred_list_indices = torch.nonzero(instance_preds, as_tuple=False).squeeze(1)\n                pred_list = [inv_label_vocab[idx.item()] for idx in pred_list_indices]\n                outputs.append({'id': fact_batch.example_ids[i], 'predictions': pred_list})\n\n    return metrics.calculate_metrics() if not infer else outputs\n\nclass MultiLabelMetrics(torch.nn.Module):\n    def __init__(self, num_classes, dev='cuda', loss=True):\n        super().__init__()\n\n        self.match = torch.zeros(num_classes, device=dev) # count no. of true positives for each label\n        self.predictions = torch.zeros(num_classes, device=dev) # count no. of true positives + false positives for each label\n        self.labels = torch.zeros(num_classes, device=dev) # count no. of true positives + false negatives for each label\n        self.run_jacc = 0 # running sum of jaccard scores\n        self.counter = 0 # count no. of batches\n        if loss:\n            self.run_loss = 0 # running sum of losses\n\n    # to be called with a batch of predictions and true labels\n    def forward(self, predictions, labels, loss=None):\n        match = predictions * labels # true positives for this batch\n\n        # increment counts\n        self.match += match.sum(dim=0)\n        self.labels += labels.sum(dim=0)\n        self.run_jacc += torch.sum(torch.logical_and(predictions, labels).sum(dim=1) / torch.logical_or(predictions, labels).sum(dim=1)).item()\n        self.counter += 1\n\n        if loss is not None:\n            self.run_loss += loss\n\n    # reset counters\n    def refresh(self):\n        self.match.fill_(0)\n        self.predictions.fill_(0)\n        self.labels.fill_(0)\n        self.run_jacc = 0\n        self.counter = 0\n        if 'run_loss' in self.__dict__:\n            self.run_loss = 0\n        return self\n\n    # calculate the metrics and return self\n    def calculate_metrics(self, refresh=True):\n        prec = self.match / self.predictions # P = TP / (TP + FP)\n        rec = self.match / self.labels # R = TP (TP + FN)\n\n        prec[prec.isnan()] = 0\n        rec[rec.isnan()] = 0\n\n        f1 = 2 * prec * rec / (prec + rec) # F1 = 2 * P * R / (P + R)\n        f1[f1.isnan()] = 0\n\n        # macro --> average across each label\n        self.macro_prec = prec.mean().item()\n        self.macro_rec = rec.mean().item()\n        self.macro_f1 = f1.mean().item()\n\n        match_total = self.match.sum().item()\n        preds_total = self.predictions.sum().item()\n        labels_total = self.labels.sum().item()\n\n        # micro --> take total counts\n        self.micro_prec = match_total / preds_total if preds_total > 0 else 0\n        self.micro_rec = match_total / labels_total\n        self.micro_f1 = 0 if self.micro_prec + self.micro_rec == 0 else 2 * self.micro_prec * self.micro_rec / (self.micro_prec + self.micro_rec)\n\n        self.jacc = self.run_jacc / self.counter\n        if 'run_loss' in self.__dict__:\n            self.loss = self.run_loss / self.counter\n\n        if refresh:\n            self.refresh()\n\n        return self\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nPreparing PyTorch environment\")\nprint(\"==================================================\")\n\nimport torch\nimport random\nfrom tqdm import tqdm\nfrom functools import partial\nimport sent2vec\nfrom gensim.models import KeyedVectors\nimport json","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_ai(message):\n    torch.autograd.set_detect_anomaly(True)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \n    \n    # SEED = hc['seed']\n    \n    # random.seed(SEED)\n    # np.random.seed(SEED)\n    # torch.manual_seed(SEED)\n    # torch.cuda.manual_seed(SEED)\n    \n    if dc['s2v_path'] is not None:\n        sent2vec_model = sent2vec.Sent2vecModel()\n        sent2vec_model.load_model(dc['s2v_path'])\n    \n    dataset = []\n    \n    # if hc['do_infer']:\n    message1 = [\"Prosecution case in brief was that Tuliya Devi, daughter of informant Bhullu Rajbhar (PW-1) was married to Gullu 5 years ago.\", \"After marriage Tuliya Devi was treated with cruelty by her husband Gullu, father-in-law Bal Chand and mother-in-law Ramwati for demand of dowry.\", \"For this reason these three accused had committed murder of Tuliya Devi on 8.5.2010 by inflicting injuries on her body at their house situate in village Chaubepur, police station- Mardah, district Ghazipur and tried to dispose of her dead body.\", \"After receiving knowledge of this incident, victim's father Bhullu (informant) had given a written report (Ex-Ka-1) to the police on the basis of which case crime no. 689 of 2010 was registered.\", \"3.In the aforesaid case inquest of the dead body was performed on 08.05.2010 and postmortem of the deceased was conducted the same evening at 9:50 pm.\", \"In the postmortem report several lacerated wounds and other marks of injuries were found on the dead body of the deceased.\", \"In postmortem (Ex-Ka-1) the doctor had opined that cause of death was haemorrhage and shock as a result of the said ante-mortem injuries.\", \"In this report approximate time of death was reported about 1 day from the time of postmortem.\", \"After completion of investigation police had submitted charge-sheet against three accused Gullu, Bal Chand and Ramwati for offences under section 498-A, 304-B IPC and under section 3/4 Dowry Prohibition Act.\", \"Accused had pleaded not guilty and claimed to be tried.\", \"5.In support of charges prosecution side examined PW-1, Bhullu (informant), PW-2 Barmati (mother of the deceased), PW-3 - Constable Sunil Kumar Singh (who prepared chik FIR and registerd case), PW-4 - Dr. Krishna Kumar Verma (who performed the postmortem), PW-5 Vinay Kumar Rai, Naib Tehsildar (for inquest report) and PW-6 Chrinjeev Nath Sinha (Investigation Officer).\", \"Hon'ble Pramod Kumar Srivastava,J.\", \"(Delivered by Hon'ble Pramod KumarSrivastava, J.)\", \"1.This appeal has been preferred against the conviction and punishment dated 28.2.2012 passed in State Vs.\", \"Gullu and others relating to Case Crime No. 689 of 2010, police station Mardah, district Ghazipur by which three accused Gullu, Bal Chand and Smt .\", \"Ramwati were convicted for charge under section 302/34 IPC and were punished with imprisonment for life and fine of Rs. 5000/- (in default of payment three months simple imprisonment).\", \"4.During trial all the accused were charged for offences under section 498-A, 304 B IPC and under section 3/4 Dowry Prohibition Act, alongwith alternative charge framed later on for offence under section 302 IPC.\", \"6.After closure of prosecution evidence statement of accused were recorded in which they denied the facts of charge as well as evidence adduced against them, without any specific averments.\", \"Defence side had examined DW-1 Madan, r/o village Chaubeypur.\", \"7.After receiving evidence from both the sides and after affording opportunity of hearing as well as considering the argument of the parties, learned Addl.\", \"Sessions Judge passed judgment dated 28.2.2012 by which all the accused were acquitted from the charges under section 498-A, 304-B IPC and 3/4 Dowry Prohibition Act, but were convicted for the charge under section 302-IPC.\", \"Thereafter the trial court had afforded an opportunity of hearing on the point of quantum of sentence to the accused and passed orders of punishment as above.\", \"Aggrieved by this judgment dated 28.2.2012 all the three accused have preferred the present appeal.\", \"8.Sri Sudist and Sri Janardan Singh Yadav appeared for appellants; and State was represented by Mrs. Usha Kiran, AGA.\", \"9.A perusal of evidence adduced during trial indicates that during post-mortem following ante-mortem injuries were found on the dead body of the deceased Tuliya Devi that were caused or occurred approximately at the time mentioned in the charge, i.e., anytime in the night of 8/9-5-2010:\", \"1.Lacerated wound left side chin 6 cm x 1 cm mussel deep chin.\", \"2.Lacerated wound from right shoulder to just below right elbow 35 cm x 2 cm x bone deep.\", \"3.Abrasion 5cm x 2 cm just laterac of left eye.\", \"4.Abrasion on top of right shoulder 8 cm x 4 cm.\", \"5.Abrasion 20cm x 10 cm right side chest.\", \"6.brasion 8 cm x 2 cm right iliac chest.\", \"7.Lacerated wound 5 cm x 3 cm at middle of front of right of leg underlying bond was fractured.\", \"8.Lacerated wound 6 cm x 3 cm just below right knee.\", \"9.Lacerated wound 8 cm x 3 cm below front of left kneel under lying bone was fractured.\", \"10.PW-4 the doctor reported that the cause of death of victim-deceased Tuliya Devi was shock and haemmerhage due to the above mentioned ante-mortem injuries.\", \"Although the defence had adduced one witness to indicate that the cause of death of the deceased may be accidental falling from the roof, and DW-1 Madan was examined in this regard but these facts could not be substantiated in the light of available evidence and circumstances.\", \"11.A perusal of the impugned judgment reveals that this finding of learned Additional Sessions Judge is correct that deceased Tuliya Devi had died due to injuries found on her body in the house of the accused-appellants at about the time mentioned in the charge, and the accused-appellants were responsible for causing such injuries to Tuliya Devi.\", \"In these circumstances, the trial court had rightly reached to the conclusion that due to the above mentioned deliberate caused injuries Tuliya Devi died, and accused-appellants are responsible for inflicting those homicidal injuries.\", \"12.Learned Additional Sessions Judge had considered facts and circumstances including evidence adduced and reached to the conclusion that though there is no conclusive evidence relating to dowry death and demand of dowry, but injuries found on body of the deceased were not accidental.\", \"Trial Court found that those injuries were homicidal, for inflicting of which accused persons were responsible, because it were only they who were present in their house when such injuries had occurred on the body of the deceased Tuliya Devi.\", \"13.Learned counsel appearing for the appellants fairly admitted the contents of facts relating to charge, namely victim Tuliya Devi having succumbed to the injuries found on her body in the house of appellants.\"]\n    \n    data_dict = [{\n        'id': 1001,\n        'text': message\n        },\n    ]\n    infer_dataset = LSIDataset(data_list = data_dict)\n    infer_dataset.preprocess()\n    # print(infer_dataset.dataset)\n    infer_dataset.sent_vectorize(sent2vec_model)\n    infer_dataset.save_data(dc['infer_cache'])\n    dataset = infer_dataset\n\n    print(\"\\nPreparing Datasets\")\n    print(\"==================================================\")\n    # sec_dataset = LSIDataset.load_data(dc['dev_cache'])\n    sec_dataset = LSIDataset(jsonl_file=dc['sec_src'])\n    sec_dataset.preprocess()\n    sec_dataset.sent_vectorize(sent2vec_model)\n    sec_dataset.save_data(dc['sec_cache'])\n    \n    vocab, label_vocab = generate_vocabs(dataset, sec_dataset, limit=hc['vocab_limit'], thresh=hc['vocab_thresh'])\n    \n    with open(dc['type_map']) as fr:\n        type_map = json.load(fr)\n    with open(dc['label_tree']) as fr:\n        label_tree = json.load(fr)\n    with open(dc['citation_network']) as fr:\n        citation_net = json.load(fr)\n    with open(dc['schemas']) as fr:\n        schemas = json.load(fr)\n    for sch in schemas.values():\n        for path in sch:\n            for i, edge in enumerate(path):\n                path[i] = tuple(path[i])\n    \n    node_vocab, edge_vocab, edge_indices, adjacency = generate_graph(label_vocab, type_map, label_tree, citation_net)\n    # sec_weights = generate_label_weights(dataset, label_vocab)\n    \n    L = len(label_vocab)\n    N = {k: len(v) for k,v in node_vocab.items()}\n    E = len(edge_vocab)\n    \n    sec_loader = torch.utils.data.DataLoader(\n        sec_dataset,\n        batch_size=len(label_vocab),\n        collate_fn=partial(\n            collate_func,\n            schemas=schemas['section'],\n            type_map=type_map,\n            node_vocab=node_vocab, \n            edge_vocab=edge_vocab,\n            adjacency=adjacency,\n            max_segments=hc['max_segments'],\n            max_segment_size=hc['max_segment_size'],\n            num_mpath_samples=hc['num_mpath_samples']\n            ),\n        pin_memory=True,\n        num_workers=2\n    )\n    for sec_batch in sec_loader:\n        break\n    \n    lsc_model = LeSICiN(\n        hc['hidden_size'],\n        L,\n        N,\n        E,\n        lambdas=hc['lambdas'],\n        thetas=hc['thetas'],\n        pthresh=hc['pthresh'],\n        drop=hc['dropout']\n        ).cuda()\n    \n    if dc['model_load'] is not None:\n        lsc_model.load_state_dict(torch.load(dc['model_load'], map_location='cuda'), strict=False)\n        print(\"loaded\")\n    \n    optimizer = torch.optim.AdamW(lsc_model.parameters(), lr=hc['opt_lr'], weight_decay=hc['opt_wt_decay'])\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=hc['sch_patience'], factor=hc['sch_factor'])\n    \n    infer_loader = torch.utils.data.DataLoader(\n    infer_dataset,\n    batch_size=hc['infer_bs'],\n    collate_fn=partial(\n        collate_func,\n        label_vocab=label_vocab,\n        max_segments=hc['max_segments'],\n        max_segment_size=hc['max_segment_size']\n        ),\n    pin_memory=True,\n    num_workers=2\n    )\n    infer_outputs =  train_dev_passs(lsc_model, optimizer, infer_loader, sec_batch, infer = True, pred_threshold=hc['pthresh'], label_vocab=label_vocab)\n    return infer_outputs[0]['predictions']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# from model.model import *\n# from data_helper import *\n# from helper import *\n\n\n\n    # infer_dataset = LSIDataset.load_data(dc['infer_cache'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install flask\n!pip install pyngrok","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyngrok import ngrok\n\nngrok.set_auth_token(\"2tNAC56VtjmtEsPPA9axlMBf3I4_2nyiQuQCiHCv982guHXCj\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nPreparing Model\")\nprint(\"==================================================\")\n\n# best_model = torch.load('/kaggle/input/ipc-recommandation/best_model.pt')\n# train_dev_pass(model, optimizer, fact_loader, sec_batch, metrics=None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pyngrok\n!pip install cors\n\n!pip install flask-cors\n!ngrok config add-authtoken 2tNAC56VtjmtEsPPA9axlMBf3I4_2nyiQuQCiHCv982guHXCj","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from flask import Flask, request, jsonify\nfrom pyngrok import ngrok\n\napp = Flask(__name__)\n\n# Load your AI model here (replace with your actual model loading code)\ndef load_model():\n    # Example: model = torch.load('model.pth')\n    return \"Your model loaded successfully!\"\n\nmodel = load_model()\n\n# Define inference function\ndef run_inference(input_data):\n    # Replace with your actual inference logic\n    # Example: prediction = model.predict(input_data)\n    return f\"Prediction for {input_data}\"\n\n# API endpoint\n@app.route('/predict', methods=['POST'])\ndef predict():\n    data = request.json\n    input_data = data.get('input')\n    print(input_data)\n    # input_case = json.loads(input_data)\n    print(type(input_data))\n    input_sents = input_data[0].split('.')\n    print(input_sents)\n    result = run_ai(input_sents)\n    # result = \"hello world\"\n    return jsonify({\"prediction\": result})\n\n\nif __name__ == '__main__':\n    # Start Flask server on port 5000\n    public_url = ngrok.connect(5000).public_url\n    print(f\" * Public URL (valid for 1 hour): {public_url}/predict\")\n    app.run(host='0.0.0.0', port=5000)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test(input_data):\n    # data = request.json\n    # input_data = data.get('input')\n    input_case = json.loads(input_data)\n    input_sents = input_data[0].split('.')\n    result = run_ai(input_sents)\n    print(result)\n    return jsonify({\"prediction\": result})\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ppsent = message.strip().lower().translate(str.maketrans('', '', string.punctuation))\n# print(ppsent)\ndata = [\n    \"He killed someone\",\n    \"He stabbed him in the eye infront of me\",\n    \"He then ran away with the knife\"\n]\n\njson_string = json.dumps(data)\nprint(json_string)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output =  test(json_string)\noutput\n\n# if hc['do_infer']:\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"infer_outputs =  train_dev_passs(lsc_model, optimizer, infer_loader, sec_batch, infer = True, pred_threshold=hc['pthresh'], label_vocab=label_vocab)\n# print(\"TEST Results || %.4f | %.4f %.4f %.4f\" % (test_mlmetrics.loss, test_mlmetrics.macro_prec, test_mlmetrics.macro_rec, test_mlmetrics.macro_f1))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(infer_outputs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target_ids = [infer_outputs[i]['id'] for i in range(len(infer_outputs))]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# with open('/content/output.txt', 'w') as fw:\n#     fw.write('\\n'.join([json.dumps(doc) for doc in infer_outputs]))\ndata = []\nwith open('/kaggle/input/ipc-recommandation/test.jsonl', 'r', encoding='utf-8') as file:\n    for line in file:\n        # Remove any extra whitespace and skip empty lines\n        line = line.strip()\n        if line:\n            data.append(json.loads(line))\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"real_dict = {item['id']: item for item in data}\npredicted_dict = {item['id']: item for item in infer_outputs}\n\n# Get all unique IDs (combines IDs from both datasets)\nall_ids = set(real_dict.keys()).union(set(predicted_dict.keys()))\n\n# Print header\nprint(f\"{'ID':<10} {'Real_IPC':<15} {'Predicted_IPC':<15}\")\nprint(\"-\" * 40)\n\n# Print each row sorted by ID (optional: remove sorted() if order doesn't matter)\nfor i, id in enumerate(sorted(all_ids)):\n    if (i > 10):\n      break\n    real_ipc = real_dict.get(id, {}).get('labels', 'N/A')  # Use 'N/A' if missing\n    predicted_ipc = predicted_dict.get(id, {}).get('predictions', 'N/A')\n    if isinstance(real_ipc, list):\n        real_ipc = ', '.join(real_ipc)\n    if isinstance(predicted_ipc, list):\n        predicted_ipc = ', '.join(predicted_ipc)\n\n    print(f\"{id:<10} \\|{real_ipc:<15}\\| {predicted_ipc:<15}\")\n    print(f\"{id:<10} \\|{real_ipc:<15}\\| {predicted_ipc:<15}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}